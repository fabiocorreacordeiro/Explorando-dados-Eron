{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size dataset:  146\n",
      "Number of POI:  18\n",
      "Number of not POI:  128\n",
      "Number of financial Features:  11\n",
      "Number of email Features:  5\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "poi = ['poi']\n",
    "financial_features = ['salary','bonus','deferral_payments','deferred_income',\n",
    "                      'exercised_stock_options','expenses','long_term_incentive',\n",
    "                      'other','restricted_stock','total_payments','total_stock_value']\n",
    "email_features = ['from_messages','from_poi_to_this_person','from_this_person_to_poi',\n",
    "                  'shared_receipt_with_poi','to_messages']\n",
    "features_list =  poi + financial_features + email_features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "print \"Size dataset: \" , len(data_dict)\n",
    "poi = 0\n",
    "not_poi = 0\n",
    "for k in data_dict:\n",
    "    if data_dict[k]['poi'] == True:\n",
    "        poi += 1\n",
    "    if data_dict[k]['poi'] == False:\n",
    "        not_poi += 1\n",
    "        \n",
    "print \"Number of POI: \" , poi\n",
    "print \"Number of not POI: \" , not_poi\n",
    "print \"Number of financial Features: \" , len(financial_features)\n",
    "print \"Number of email Features: \" , len(email_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------    \n",
    "### Task 2: Remove outliers\n",
    "\n",
    "# remove 'TOTAL' from dictionary\n",
    "del data_dict['TOTAL']\n",
    "\n",
    "# remove 'THE TRAVEL AGENCY IN THE PARK' from dictionary\n",
    "del data_dict['THE TRAVEL AGENCY IN THE PARK']\n",
    "\n",
    "# remove negative values from 'restricted_stock'\n",
    "for person in data_dict:\n",
    "        if data_dict[person]['restricted_stock'] < 0 and data_dict[person]['restricted_stock'] != 'NaN':\n",
    "            data_dict[person]['restricted_stock'] = 'NaN'\n",
    "\n",
    "# remove negative values from 'deferral_payments'\n",
    "for person in data_dict:\n",
    "        if data_dict[person]['deferral_payments'] < 0 and data_dict[person]['deferral_payments'] != 'NaN':\n",
    "            data_dict[person]['deferral_payments'] = 'NaN'\n",
    "\n",
    "# remove negative values from 'total_stock_value'\n",
    "for person in data_dict:\n",
    "        if data_dict[person]['total_stock_value'] < 0 and data_dict[person]['total_stock_value'] != 'NaN':\n",
    "            data_dict[person]['total_stock_value'] = 'NaN'\n",
    "            \n",
    "# Remove 'restricted_stock_deferred' and 'loan_advances' from the features, few relevant data available\n",
    "\n",
    "# Remove 'director_fee' because there is only non-POI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E\n",
      "{'salary': 'NaN', 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 'NaN', 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': 'NaN', 'total_stock_value': 'NaN', 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 'NaN', 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n"
     ]
    }
   ],
   "source": [
    "# Checking if had some person without value\n",
    "not_NaN_data = {}\n",
    "for key in data_dict:\n",
    "    not_NaN_feature = 0\n",
    "    for feature in data_dict[key]:\n",
    "        if data_dict[key][feature] != 'NaN':\n",
    "            not_NaN_feature += 1\n",
    "    not_NaN_data[key] = not_NaN_feature\n",
    "\n",
    "for k in not_NaN_data:\n",
    "    if not_NaN_data[k] == 1:\n",
    "        print k\n",
    "        print data_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove 'THE TRAVEL AGENCY IN THE PARK' from dictionary\n",
    "del data_dict['LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "### The messages to and from POI are an absolute measure, let's create new features that are a ratio of the total messages.\n",
    "def new_feature_ratio(new_feature, numerator, denominator):    \n",
    "    for key in data_dict:\n",
    "        if data_dict[key][denominator] != 'NaN' and data_dict[key][numerator] != \"NaN\":\n",
    "            data_dict[key][new_feature] = float(data_dict[key][numerator]) / float(data_dict[key][denominator])\n",
    "        else:\n",
    "            data_dict[key][new_feature] = \"NaN\"\n",
    "    features_list.append(new_feature)\n",
    "    \n",
    "### Feature - 'from_this_person_to_poi_ratio'\n",
    "new_feature_ratio('from_this_person_to_poi_ratio', 'from_this_person_to_poi', 'from_messages')\n",
    "\n",
    "### Feature - 'from_poi_to_this_person_ratio'\n",
    "new_feature_ratio('from_poi_to_this_person_ratio', 'from_poi_to_this_person', 'to_messages')\n",
    "    \n",
    "### Feature - 'bonus_ratio'\n",
    "new_feature_ratio('bonus_ratio', 'bonus', 'salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Put features with \"long tail\" in log10 scale\n",
    "features_list_log = ['salary','bonus','deferral_payments','exercised_stock_options',\n",
    "                     'expenses','long_term_incentive','other','restricted_stock',\n",
    "                     'total_payments','total_stock_value', 'from_messages', \n",
    "                     'from_poi_to_this_person', 'from_this_person_to_poi', \n",
    "                     'shared_receipt_with_poi', 'bonus_ratio']\n",
    "features_list_log = []\n",
    "for n in range(1,len(features_list_log)):\n",
    "    for person in my_dataset:\n",
    "        if my_dataset[person][features_list_log[n]] != \"NaN\":\n",
    "            if my_dataset[person][features_list_log[n]] >= 0:\n",
    "                if my_dataset[person][features_list_log[n]] == 0:\n",
    "                    my_dataset[person][features_list_log[n]] = 0\n",
    "            else:\n",
    "                my_dataset[person][features_list_log[n]] = np.log10(my_dataset[person][features_list_log[n]]*-1)\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Put all features in same reange (0,1)\n",
    "for n in range(0,len(features[0])):\n",
    "    feature = []\n",
    "    for person in range(0,len(features)):\n",
    "        feature.append(features[person][n])\n",
    "    feature = np.array(feature).reshape(-1,1)\n",
    "    feature = scaler.fit_transform(feature)\n",
    "    for person in range(0,len(features)):\n",
    "        features[person][n] = feature[person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_NB = GaussianNB()\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':[1, 10], 'degree': [2,10]}\n",
    "svr = svm.SVC()\n",
    "clf_SVM = GridSearchCV(svr, parameters, scoring = 'f1')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'criterion':('gini', 'entropy'), 'splitter':('best', 'random'), 'min_samples_split':[2,200]}\n",
    "svr = DecisionTreeClassifier()\n",
    "clf_tree = GridSearchCV(svr, parameters, scoring = 'f1')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators': [2,20], 'criterion':('gini', 'entropy'), 'min_samples_split':[2,200]}\n",
    "svr = RandomForestClassifier()\n",
    "clf_randon_forest = GridSearchCV(svr, parameters, scoring = 'f1')\n",
    "\n",
    "classifiers = {\"clf_NB\": clf_NB,\n",
    "               \"clf_SVM\": clf_SVM,\n",
    "               \"clf_tree\": clf_tree,\n",
    "               \"clf_randon_forest\": clf_randon_forest}\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "### Using K-fold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def train_test_StratifiedKFold(clf, k_best, features):\n",
    "    # Enter a classifier and the number of the k-best features and the function return\n",
    "    # the classifier and validation metrics\n",
    "    acc = []\n",
    "    pre = []\n",
    "    rec = []\n",
    "    f = []\n",
    "    skf = StratifiedKFold(2, shuffle=True)\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        features_train = [features[ii] for ii in train_index] \n",
    "        labels_train = [labels[ii] for ii in train_index]\n",
    "        features_test = [features[ii] for ii in test_index]\n",
    "        labels_test = [labels[ii] for ii in test_index]\n",
    "        \n",
    "        skb = SelectKBest(f_classif, k = k_best)\n",
    "        pipe = make_pipeline(skb, clf)\n",
    "        pipe.fit(features_train, labels_train)\n",
    "        labels_pred = pipe.predict(features_test)\n",
    "        acc.append(accuracy_score (labels_test, labels_pred))\n",
    "        pre_rec_f = precision_recall_fscore_support (labels_test, labels_pred)\n",
    "        try:\n",
    "            pre.append(pre_rec_f[0][1])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            rec.append(pre_rec_f[1][1])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            f.append(pre_rec_f[2][1])\n",
    "        except:\n",
    "            pass\n",
    "    return [pipe, np.mean(acc), np.mean(pre), np.mean(rec), np.mean(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test k-best  1\n",
      "Accuracy:  [0.90238654147104858, 'clf_tree', 8]\n",
      "Precision:  [0.69999999999999996, 'clf_NB', 2]\n",
      "Reccal:  [0.5, 'clf_tree', 8]\n",
      "f Score:  [0.48333333333333328, 'clf_tree', 8]\n",
      "\n",
      "Test k-best  2\n",
      "Accuracy:  [0.88125978090766832, 'clf_randon_forest', 8]\n",
      "Precision:  [0.66666666666666663, 'clf_SVM', 15]\n",
      "Reccal:  [0.5, 'clf_NB', 5]\n",
      "f Score:  [0.48459383753501406, 'clf_NB', 5]\n",
      "\n",
      "Test k-best  3\n",
      "Accuracy:  [0.88820422535211274, 'clf_SVM', 2]\n",
      "Precision:  [0.75, 'clf_randon_forest', 9]\n",
      "Reccal:  [0.55555555555555558, 'clf_NB', 12]\n",
      "f Score:  [0.47268907563025209, 'clf_NB', 12]\n",
      "\n",
      "Test k-best  4\n",
      "Accuracy:  [0.90189749608763692, 'clf_randon_forest', 12]\n",
      "Precision:  [0.75, 'clf_randon_forest', 16]\n",
      "Reccal:  [0.5, 'clf_tree', 9]\n",
      "f Score:  [0.44385026737967914, 'clf_randon_forest', 12]\n",
      "\n",
      "Test k-best  5\n",
      "Accuracy:  [0.88810641627543041, 'clf_SVM', 6]\n",
      "Precision:  [1.0, 'clf_SVM', 6]\n",
      "Reccal:  [0.66666666666666674, 'clf_tree', 12]\n",
      "f Score:  [0.47727272727272729, 'clf_NB', 6]\n",
      "\n",
      "Test PCA 1\n",
      "Accuracy:  [0.88810641627543041, 'clf_SVM', 9]\n",
      "Precision:  [1.0, 'clf_SVM', 9]\n",
      "Reccal:  [0.3888888888888889, 'clf_NB', 11]\n",
      "f Score:  [0.39649122807017545, 'clf_tree', 3]\n",
      "\n",
      "Test PCA 2\n",
      "Accuracy:  [0.88820422535211274, 'clf_SVM', 6]\n",
      "Precision:  [1.0, 'clf_SVM', 15]\n",
      "Reccal:  [0.44444444444444442, 'clf_tree', 6]\n",
      "f Score:  [0.36024844720496896, 'clf_NB', 6]\n",
      "\n",
      "Test PCA 3\n",
      "Accuracy:  [0.88820422535211274, 'clf_SVM', 12]\n",
      "Precision:  [0.75, 'clf_SVM', 12]\n",
      "Reccal:  [0.5, 'clf_NB', 14]\n",
      "f Score:  [0.42241379310344829, 'clf_NB', 14]\n",
      "\n",
      "Test PCA 4\n",
      "Accuracy:  [0.89514866979655716, 'clf_tree', 10]\n",
      "Precision:  [1.0, 'clf_SVM', 14]\n",
      "Reccal:  [0.3888888888888889, 'clf_tree', 10]\n",
      "f Score:  [0.46078431372549022, 'clf_tree', 10]\n",
      "\n",
      "Test PCA 5\n",
      "Accuracy:  [0.89524647887323949, 'clf_SVM', 15]\n",
      "Precision:  [0.7857142857142857, 'clf_tree', 8]\n",
      "Reccal:  [0.55555555555555558, 'clf_NB', 14]\n",
      "f Score:  [0.43181818181818177, 'clf_tree', 8]\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------\n",
    "# Now we will test the best classifiers\n",
    "\n",
    "best_clf = [None, None, None]\n",
    "\n",
    "# We will test all combination of the 4 algoritms and k-best features (k from 1 to 19). \n",
    "# For each metric (accuracy, precision, recall and f) we will print the best combination.\n",
    "# We will try 5 times to be sure to choose the best combination\n",
    "for test in range(1,6):\n",
    "    max_acc = [0, 'NaN', 'NaN']\n",
    "    max_pre = [0, 'NaN', 'NaN']\n",
    "    max_rec = [0, 'NaN', 'NaN']\n",
    "    max_f = [0, 'NaN', 'NaN']\n",
    "    for algor in classifiers:\n",
    "        for k_best in range(1, 17): #20):\n",
    "            preview_clf, acc, pre, rec, f = train_test_StratifiedKFold(classifiers[algor], k_best, features)\n",
    "            if acc > max_acc[0]:\n",
    "                max_acc = [acc, algor, k_best]\n",
    "            if pre > max_pre[0]:\n",
    "                max_pre = [pre, algor, k_best]\n",
    "            if rec > max_rec[0]:\n",
    "                max_rec = [rec, algor, k_best]\n",
    "            if f > max_f[0]:\n",
    "                max_f = [f, algor, k_best]\n",
    "                best_clf = ['k-best', max_f, preview_clf]\n",
    "\n",
    "    print \"\"\n",
    "    print \"Test k-best \", test\n",
    "    print 'Accuracy: ', max_acc\n",
    "    print 'Precision: ', max_pre\n",
    "    print 'Reccal: ', max_rec\n",
    "    print 'f Score: ', max_f\n",
    "\n",
    "### We will do the same but decomponding the features using PCA (nÂ° of componnents 1 to 19)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for test in range(1,6):\n",
    "    max_acc = [0, 'NaN', 'NaN']\n",
    "    max_pre = [0, 'NaN', 'NaN']\n",
    "    max_rec = [0, 'NaN', 'NaN']\n",
    "    max_f = [0, 'NaN', 'NaN']\n",
    "    for algor in classifiers:\n",
    "        for n_comp in range(1, 17): #20):\n",
    "            pca = PCA(n_components = n_comp)\n",
    "            pipe = make_pipeline(pca, classifiers[algor])\n",
    "            #pca_features = pca.fit_transform(features)\n",
    "            preview_clf, acc, pre, rec, f = train_test_StratifiedKFold(pipe, \"all\", features)\n",
    "            if acc > max_acc[0]:\n",
    "                max_acc = [acc, algor, n_comp]\n",
    "            if pre > max_pre[0]:\n",
    "                max_pre = [pre, algor, n_comp]\n",
    "            if rec > max_rec[0]:\n",
    "                max_rec = [rec, algor, n_comp]\n",
    "            if f > max_f[0]:\n",
    "                max_f = [f, algor, n_comp]\n",
    "            if f > best_clf[1][0]:\n",
    "                best_clf = ['PCA', max_f, preview_clf]\n",
    "\n",
    "    print \"\"\n",
    "    print \"Test PCA\", test\n",
    "    print 'Accuracy: ', max_acc\n",
    "    print 'Precision: ', max_pre\n",
    "    print 'Reccal: ', max_rec\n",
    "    print 'f Score: ', max_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f classi:  [0.47727272727272729, 'clf_NB', 6]\n",
      "K-best or PCA:  k-best\n",
      "Classifier:\n",
      "Pipeline(steps=[('selectkbest', SelectKBest(k=6, score_func=<function f_classif at 0x0000000008D03C18>)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\n",
      "Pipeline(steps=[('selectkbest', SelectKBest(k=6, score_func=<function f_classif at 0x0000000008D03C18>)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.86667\tPrecision: 0.50000\tRecall: 0.50000\tF1: 0.50000\tF2: 0.50000\n",
      "\tTotal predictions:   15\tTrue positives:    1\tFalse positives:    1\tFalse negatives:    1\tTrue negatives:   12\n",
      "\n",
      "\n",
      "--- 620.782999992 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "print \"f classi: \", best_clf[1]\n",
    "print \"K-best or PCA: \", best_clf[0]\n",
    "print \"Classifier:\"\n",
    "print best_clf[2]\n",
    "\n",
    "\n",
    "### The best classifier is\n",
    "clf = best_clf[2]\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \"\"\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)\n",
    "\n",
    "print \"\"\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
